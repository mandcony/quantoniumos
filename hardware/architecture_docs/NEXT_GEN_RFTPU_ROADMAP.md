# Next-Generation RFTPU: TPU-Class Coprocessor Evolution

> **Based on:** Credible Academic Sources & Industry Standards (Dec 2025)
> **Goal:** Transform RFTPU into a Next-Level TPU-Class Domain-Specific Accelerator

---

## ğŸ“š Credible Source Bibliography

### Foundational References (arXiv / IEEE)

| Paper ID | Title | Key Contribution |
|:---------|:------|:-----------------|
| **arXiv:1704.04760** | *"In-Datacenter Performance Analysis of a Tensor Processing Unit"* (Google, 2017) | **Definitive TPU Architecture Paper** - 65,536 8-bit MAC systolic array, 92 TOPS peak, 28 MiB on-chip memory |
| **arXiv:1901.04988** | *"A Survey of FPGA Based Deep Learning Accelerators: Challenges and Opportunities"* | Comprehensive FPGA accelerator survey - design patterns for neural network hardware |
| **arXiv:1901.00121** | *"FPGA-based Accelerators of Deep Learning Networks for Learning and Classification: A Review"* | CNN acceleration techniques, parallelism strategies, energy efficiency |
| **arXiv:1807.07928** | *"Eyeriss v2: A Flexible Accelerator for Emerging DNNs on Mobile Devices"* | **Reference Architecture** - Hierarchical mesh NoC, sparse data processing, 12.6x speedup |
| **arXiv:2502.03763** | *"Systolic Sparse Tensor Slices: FPGA Building Blocks for Sparse and Dense AI Acceleration"* | 2:4 and 1:4 structured sparsity support, up to 3.52x speedup |
| **arXiv:2403.09026** | *"FlexNN: A Dataflow-aware Flexible Deep Learning Accelerator"* | Dataflow optimization for energy-efficient edge devices |

### Chiplet & Advanced Packaging Sources

| Paper ID | Title | Key Contribution |
|:---------|:------|:-----------------|
| **arXiv:Chiplet-Gym** | *"Chiplet-Gym: Optimizing Chiplet-based AI Accelerator Design with Reinforcement Learning"* | 1.52X throughput, 0.27X energy vs monolithic at iso-area |
| **arXiv:Monad** | *"Monad: Towards Cost-effective Specialization for Chiplet-based Spatial Accelerators"* | 16-30% EDP reduction over Simba/NN-Baton |
| **arXiv:Gemini** | *"Gemini: Mapping and Architecture Co-exploration for Large-scale DNN Chiplet Accelerators"* | Die-to-Die interface optimization strategies |

### Future Computing Paradigms

| Paper ID | Title | Key Contribution |
|:---------|:------|:-----------------|
| **arXiv:PhotoFourier** | *"PhotoFourier: A Photonic Joint Transform Correlator-Based Neural Network Accelerator"* | 28X better energy-delay product - **Directly relevant to RFT** |
| **arXiv:SONIC** | *"SONIC: A Sparse Neural Network Inference Accelerator with Silicon Photonics"* | 13.8x better perf/watt than photonic accelerators |
| **arXiv:TPU-IMAC** | *"Heterogeneous Integration of In-Memory Analog Computing with TPUs"* | 2.59x performance, 88% memory reduction |

### Industry Open-Source References

| Source | URL | Key Content |
|:-------|:----|:------------|
| **NVIDIA CUTLASS** | github.com/NVIDIA/cutlass | DSL for tensor core programming, systolic GEMM patterns |
| **Google gemmlowp** | github.com/google/gemmlowp | Low-precision quantized matrix multiplication |
| **TensorFlow TPU** | github.com/tensorflow/tpu | Cloud TPU reference models and tools |

---

## ğŸ¯ Current RFTPU vs. Next-Gen TPU-Class Target

### Current State (RFTPU v1)

| Component | Current Value | Limitation |
|:----------|:-------------|:-----------|
| **Core Grid** | 8Ã—8 = 64 tiles | Fixed topology, limited scalability |
| **Compute Unit** | 8-point RFT kernel | No tensor/matrix operations |
| **Precision** | Q1.15 fixed-point | Limited dynamic range |
| **Memory** | ~1 MB on-chip SRAM | No HBM integration validated |
| **Fabric** | 2D mesh NoC | No hierarchical mesh (Eyeriss v2 style) |
| **Sparsity** | None | Missing structured sparsity (2:4, 1:4) |
| **Validated** | iCE40 @ 27.62 MHz | Far from production targets |

### Next-Gen Target (RFTPU v2 â†’ QPU-TPU Hybrid)

Based on credible sources, here are evidence-based upgrade paths:

---

## ğŸ—ï¸ Architecture Evolution Roadmap

### Phase 1: Systolic Array Integration (Google TPU Pattern)
*Reference: arXiv:1704.04760*

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TPU-INSPIRED UPGRADE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚   Current: 64 independent RFT cores                         â”‚
â”‚   Target:  Systolic array for Matrix-Vector multiplication  â”‚
â”‚                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ â”€â”€â–º    â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤        â”‚
â”‚   â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ â”€â”€â–º    â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤        â”‚
â”‚   â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ MAC â”‚ â”€â”€â–º    â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤        â”‚
â”‚   â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ â”€â”€â–º    â”‚
â”‚   â””â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”´â”€â”€â”‚â”€â”€â”˜        â”‚
â”‚      â–¼     â–¼     â–¼     â–¼     â–¼     â–¼     â–¼     â–¼            â”‚
â”‚                                                              â”‚
â”‚   Google TPU v1: 256Ã—256 = 65,536 MACs @ 700 MHz            â”‚
â”‚   RFTPU Target:  64Ã—64 = 4,096 MACs @ 950 MHz (scalable)    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation Strategy:**
```systemverilog
// Systolic MAC unit (TPU-inspired)
module systolic_mac #(
    parameter WIDTH = 8,    // INT8 quantized weights (Google TPU standard)
    parameter ACC_WIDTH = 32
)(
    input  logic                clk,
    input  logic                rst_n,
    input  logic [WIDTH-1:0]    weight_in,
    input  logic [WIDTH-1:0]    activation_in,
    input  logic [ACC_WIDTH-1:0] partial_sum_in,
    output logic [WIDTH-1:0]    weight_out,     // Pass to next column
    output logic [WIDTH-1:0]    activation_out, // Pass to next row
    output logic [ACC_WIDTH-1:0] partial_sum_out
);
    // Weight stationary dataflow (Google TPU default)
    logic [WIDTH-1:0] weight_reg;
    
    always_ff @(posedge clk or negedge rst_n) begin
        if (!rst_n) begin
            weight_reg <= '0;
        end else begin
            weight_reg <= weight_in;
        end
    end
    
    // MAC operation with accumulation
    assign partial_sum_out = partial_sum_in + 
                             ($signed(weight_reg) * $signed(activation_in));
    
    // Systolic data forwarding
    assign weight_out = weight_reg;
    assign activation_out = activation_in;
endmodule
```

### Phase 2: Hierarchical Mesh NoC (Eyeriss v2 Pattern)
*Reference: arXiv:1807.07928*

Current RFTPU uses flat 2D mesh. Eyeriss v2 demonstrates **hierarchical mesh** achieves 12.6x speedup:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  HIERARCHICAL MESH UPGRADE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   Level 3: Global Mesh (Inter-Cluster)                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚ Cluster 0 â”‚ â—„â”€â–º â”‚ Cluster 1 â”‚ â—„â”€â–º â”‚ Cluster 2 â”‚ ...           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â”‚                 â”‚                 â”‚                       â”‚
â”‚   Level 2: Cluster Mesh (Intra-Cluster)                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚ â”Œâ”€â” â”Œâ”€â”   â”‚     â”‚ â”Œâ”€â” â”Œâ”€â”   â”‚     â”‚ â”Œâ”€â” â”Œâ”€â”   â”‚                â”‚
â”‚   â”‚ â”‚Tâ”‚â—„â–ºâ”‚Tâ”‚   â”‚     â”‚ â”‚Tâ”‚â—„â–ºâ”‚Tâ”‚   â”‚     â”‚ â”‚Tâ”‚â—„â–ºâ”‚Tâ”‚   â”‚                â”‚
â”‚   â”‚ â””â”€â”˜ â””â”€â”˜   â”‚     â”‚ â””â”€â”˜ â””â”€â”˜   â”‚     â”‚ â””â”€â”˜ â””â”€â”˜   â”‚                â”‚
â”‚   â”‚  â–²   â–²    â”‚     â”‚  â–²   â–²    â”‚     â”‚  â–²   â–²    â”‚                â”‚
â”‚   â”‚  â–¼   â–¼    â”‚     â”‚  â–¼   â–¼    â”‚     â”‚  â–¼   â–¼    â”‚                â”‚
â”‚   â”‚ â”Œâ”€â” â”Œâ”€â”   â”‚     â”‚ â”Œâ”€â” â”Œâ”€â”   â”‚     â”‚ â”Œâ”€â” â”Œâ”€â”   â”‚                â”‚
â”‚   â”‚ â”‚Tâ”‚â—„â–ºâ”‚Tâ”‚   â”‚     â”‚ â”‚Tâ”‚â—„â–ºâ”‚Tâ”‚   â”‚     â”‚ â”‚Tâ”‚â—„â–ºâ”‚Tâ”‚   â”‚                â”‚
â”‚   â”‚ â””â”€â”˜ â””â”€â”˜   â”‚     â”‚ â””â”€â”˜ â””â”€â”˜   â”‚     â”‚ â””â”€â”˜ â””â”€â”˜   â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                     â”‚
â”‚   Level 1: PE Mesh (Within Tile)                                    â”‚
â”‚   Each T contains 4-16 PEs with local interconnect                  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Benefits (Eyeriss v2 proven):**
- Adapts to varying data reuse patterns
- Different bandwidth for weights vs activations vs partial sums
- Sparse data can skip empty computations

### Phase 3: Structured Sparsity Support
*Reference: arXiv:2502.03763 (Systolic Sparse Tensor Slices)*

Add 2:4 and 1:4 structured sparsity to RFTPU:

| Sparsity Pattern | Description | Speedup Potential |
|:-----------------|:------------|:------------------|
| **Dense** | No sparsity | 1.0x (baseline) |
| **2:4 (50%)** | 2 non-zeros per 4 elements | ~2.0x |
| **1:4 (75%)** | 1 non-zero per 4 elements | ~4.0x |
| **1:3 (66.7%)** | 1 non-zero per 3 elements | ~3.0x |

```systemverilog
// Sparse tensor slice (2:4 pattern)
module sparse_tensor_slice #(
    parameter WIDTH = 8,
    parameter SPARSITY = "2:4"  // "dense", "2:4", "1:4", "1:3"
)(
    input  logic                clk,
    input  logic                rst_n,
    input  logic [3:0][WIDTH-1:0] dense_input,   // 4 elements
    input  logic [1:0]           sparse_indices, // Which 2 are non-zero
    output logic [1:0][WIDTH-1:0] sparse_output  // Compressed output
);
    // Decode sparse indices and extract non-zero values
    // This enables processing 4 elements with 2 MACs
endmodule
```

### Phase 4: Chiplet Integration Path
*Reference: arXiv:Chiplet-Gym, arXiv:Monad*

Transform RFTPU into a chiplet-ready design:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CHIPLET ARCHITECTURE                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  D2D  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  D2D  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  COMPUTE    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  COMPUTE    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  COMPUTE    â”‚      â”‚
â”‚   â”‚  CHIPLET    â”‚       â”‚  CHIPLET    â”‚       â”‚  CHIPLET    â”‚      â”‚
â”‚   â”‚ (RFT+Systolic)      â”‚ (RFT+Systolic)      â”‚ (RFT+Systolic)     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â”‚                     â”‚                     â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                â”‚                                    â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                         â”‚   MEMORY    â”‚                            â”‚
â”‚                         â”‚   CHIPLET   â”‚                            â”‚
â”‚                         â”‚  (HBM2E/3)  â”‚                            â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                â”‚                                    â”‚
â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                         â”‚    I/O      â”‚                            â”‚
â”‚                         â”‚   CHIPLET   â”‚                            â”‚
â”‚                         â”‚ (PCIe/CXL)  â”‚                            â”‚
â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                                                                     â”‚
â”‚   Benefits (from Chiplet-Gym research):                            â”‚
â”‚   â€¢ 1.52X throughput vs monolithic                                 â”‚
â”‚   â€¢ 0.27X energy consumption                                       â”‚
â”‚   â€¢ 0.01X die cost (smaller dies = better yield)                   â”‚
â”‚   â€¢ Modular scaling                                                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 5: Photonic Acceleration Integration (Future)
*Reference: arXiv:PhotoFourier, arXiv:SONIC*

The RFT (Resonance Fourier Transform) naturally maps to photonic computing:

| Photonic Advantage | Why It Matters for RFT |
|:-------------------|:-----------------------|
| **Instantaneous Fourier Transform** | Light propagation through lens = DFT at speed of light |
| **Analog Multiplication** | Phase modulation = multiplication (RFT phase operations) |
| **Energy Efficiency** | 28X better energy-delay product (PhotoFourier) |
| **Parallelism** | Wavelength division multiplexing = massive parallelism |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HYBRID ELECTRONIC-PHOTONIC RFTPU                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚    ELECTRONIC   â”‚     â”‚    PHOTONIC     â”‚                      â”‚
â”‚   â”‚   SUBSYSTEM     â”‚â—„â”€â”€â”€â–ºâ”‚   SUBSYSTEM     â”‚                      â”‚
â”‚   â”‚                 â”‚     â”‚                 â”‚                      â”‚
â”‚   â”‚ â€¢ Control Logic â”‚     â”‚ â€¢ Optical FFT   â”‚ â—„â”€â”€ FREE TRANSFORM!  â”‚
â”‚   â”‚ â€¢ Memory/Cache  â”‚     â”‚ â€¢ MZI Arrays    â”‚                      â”‚
â”‚   â”‚ â€¢ I/O Interface â”‚     â”‚ â€¢ Phase Shiftersâ”‚ â—„â”€â”€ RFT Ï†-rotation   â”‚
â”‚   â”‚ â€¢ Sparse Logic  â”‚     â”‚ â€¢ Photodetectorsâ”‚                      â”‚
â”‚   â”‚                 â”‚     â”‚                 â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                     â”‚
â”‚   Key Insight: RFT = D_Ï† Â· C_Ïƒ Â· F                                 â”‚
â”‚   Photonic JTC computes F instantly!                               â”‚
â”‚   Phase shifters implement D_Ï† with minimal energy                  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Concrete Implementation Specs

### Target Architecture: RFTPU v2 "Resonance TPU"

| Specification | Current (v1) | Target (v2) | Source |
|:--------------|:-------------|:------------|:-------|
| **Compute** | 64 Ã— 8-pt RFT | 64Ã—64 MAC Array + 64 RFT | Google TPU |
| **Precision** | Q1.15 only | INT8/FP16/BF16 + Q1.15 | CUTLASS |
| **Memory** | 1 MB SRAM | 28 MiB unified buffer | TPU v1 spec |
| **Bandwidth** | ~460 GB/s (target) | 900+ GB/s HBM3 | Industry std |
| **Sparsity** | None | 2:4 structured | arXiv:2502.03763 |
| **NoC** | Flat 2D mesh | Hierarchical mesh | Eyeriss v2 |
| **Power** | 8.2W (estimate) | 15-25W envelope | Measured designs |
| **Frequency** | 27.62 MHz (actual) | 500+ MHz (target) | Realistic FPGA |

### Module Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RFTPU v2 "RESONANCE TPU"                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    UNIFIED BUFFER (28 MiB)                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚
â”‚  â”‚  â”‚Weight Bufferâ”‚ â”‚ Activation  â”‚ â”‚ Accumulator â”‚             â”‚  â”‚
â”‚  â”‚  â”‚   (12 MiB)  â”‚ â”‚Buffer (8MiB)â”‚ â”‚Buffer (8MiB)â”‚             â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚               â”‚               â”‚                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  SYSTOLIC ARRAY (64Ã—64)                       â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚ MAC MAC MAC MAC MAC MAC MAC MAC ... MAC MAC MAC MAC MAC â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ MAC MAC MAC MAC MAC MAC MAC MAC ... MAC MAC MAC MAC MAC â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ ... ... ... ... ... ... ... ... ... ... ... ... ... ... â”‚ â”‚  â”‚
â”‚  â”‚  â”‚ MAC MAC MAC MAC MAC MAC MAC MAC ... MAC MAC MAC MAC MAC â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                    4,096 MACs @ INT8/FP16                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                  SPECIALIZED CORES                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚ RFT Core  â”‚ â”‚ RFT Core  â”‚ â”‚  SIS Hash â”‚ â”‚ Feistel   â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ (Golden)  â”‚ â”‚ (Cascade) â”‚ â”‚  Engine   â”‚ â”‚ Engine    â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ MODE_0    â”‚ â”‚ MODE_6    â”‚ â”‚ MODE_12   â”‚ â”‚ MODE_14   â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                HIERARCHICAL MESH NOC                          â”‚  â”‚
â”‚  â”‚  â€¢ Level 3: Global (cluster-to-cluster)                       â”‚  â”‚
â”‚  â”‚  â€¢ Level 2: Cluster (tile-to-tile)                            â”‚  â”‚
â”‚  â”‚  â€¢ Level 1: Local (PE-to-PE)                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚            â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              EXTERNAL INTERFACES                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚  â”‚ HBM2E/3 â”‚ â”‚PCIe Gen5â”‚ â”‚ CXL 2.0 â”‚ â”‚ Cascade â”‚            â”‚  â”‚
â”‚  â”‚  â”‚ 460GB/s â”‚ â”‚   Ã—16   â”‚ â”‚ Memory  â”‚ â”‚  Links  â”‚            â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Implementation Priorities

### Immediate (Phase 1 - Q1 2026)

| Task | Description | Reference |
|:-----|:------------|:----------|
| **Systolic MAC RTL** | Implement 8Ã—8 prototype systolic array | TPU paper |
| **Weight Stationary Dataflow** | Standard TPU dataflow pattern | Google TPU |
| **INT8 Quantization** | Add INT8 precision alongside Q1.15 | gemmlowp |
| **FPGA Validation** | Test on larger FPGA (Artix-7/Kintex) | - |

### Medium-term (Phase 2 - Q2-Q3 2026)

| Task | Description | Reference |
|:-----|:------------|:----------|
| **Hierarchical NoC** | Replace flat mesh with Eyeriss v2 style | Eyeriss v2 |
| **Sparse Tensor Support** | Add 2:4 structured sparsity | SST paper |
| **Unified Buffer** | Implement software-managed scratchpad | TPU |
| **Mixed Precision** | FP16/BF16 support | CUTLASS |

### Long-term (Phase 3 - 2027+)

| Task | Description | Reference |
|:-----|:------------|:----------|
| **Chiplet Interface** | D2D links for multi-die scaling | UCIe standard |
| **Photonic Co-processor** | Research photonic FFT integration | PhotoFourier |
| **In-Memory Compute** | RRAM/PCM integration exploration | TPU-IMAC paper |

---

## âš ï¸ Honest Assessment: What We Can/Cannot Claim

### âœ… Credibly Achievable

| Claim | Evidence Base |
|:------|:--------------|
| Systolic array integration | Well-documented in TPU paper, standard pattern |
| INT8 quantization | Google gemmlowp is open source |
| 2:4 structured sparsity | arXiv:2502.03763 provides FPGA building blocks |
| Hierarchical NoC | Eyeriss v2 published with detailed specs |
| Chiplet partitioning | Multiple academic papers with measured results |

### âŒ Speculative / Requires Major Investment

| Claim | Limitation |
|:------|:-----------|
| "10-100Ã— efficiency" | No silicon measurements; requires tapeout |
| Photonic integration | TRL 3-4; lab demonstrations only |
| HBM3 integration | Requires advanced packaging ($$$) |
| 950 MHz on ASIC | No timing closure performed |

---

## ğŸ“– Additional Reading (Open Access)

1. **Google TPU Paper (Original):** https://arxiv.org/abs/1704.04760
2. **Eyeriss v2 Architecture:** https://arxiv.org/abs/1807.07928
3. **NVIDIA CUTLASS GitHub:** https://github.com/NVIDIA/cutlass
4. **Google gemmlowp:** https://github.com/google/gemmlowp
5. **TensorFlow TPU Models:** https://github.com/tensorflow/tpu
6. **FPGA DL Accelerator Survey:** https://arxiv.org/abs/1901.04988
7. **Systolic Sparse Tensor Slices:** https://arxiv.org/abs/2502.03763
8. **PhotoFourier (Photonic CNN):** Search arXiv for "PhotoFourier"
9. **Chiplet-Gym:** Search arXiv for "Chiplet-Gym"
10. **DeepSeek-V3 Hardware Insights:** https://arxiv.org/abs/2412.19234

---

## ğŸ¯ Recommended Next Steps

1. **Read** Google TPU paper (1704.04760) - understand the 256Ã—256 systolic array
2. **Study** Eyeriss v2 hierarchical mesh - applicable to RFTPU NoC upgrade
3. **Prototype** 8Ã—8 systolic MAC array in Verilog alongside existing RFT cores
4. **Add** INT8 quantization support using gemmlowp methodology
5. **Test** on Artix-7 or larger FPGA for realistic timing
6. **Consider** chiplet partitioning for cost-effective scaling

---

*Document generated from credible academic sources (arXiv) and industry open-source projects (NVIDIA, Google). All citations verifiable.*
