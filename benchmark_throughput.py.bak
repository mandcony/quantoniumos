#!/usr/bin/env python3
"""
QuantoniumOS Automated Throughput Benchmark
Generates CI artifact for performance validation
"""

import hashlib
import json
import os
import sys
import time
from datetime import datetime
from typing import Any, Dict, List

# Add core modules to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'core'))

def benchmark_sha256_throughput(data_size_mb: int = 100, runs: int = 5) -> Dict[str, Any]:
    """Benchmark SHA-256 throughput performance"""
    data_size_bytes = data_size_mb * 1024 * 1024
    test_data = b'a' * data_size_bytes
    
    print(f"Benchmarking SHA-256 with {data_size_mb}MB data, {runs} runs...")
    
    times = []
    for run in range(runs):
        print(f"  Run {run + 1}/{runs}...", end='', flush=True)
        start = time.time()
        hashlib.sha256(test_data).hexdigest()
        end = time.time()
        duration = end - start
        times.append(duration)
        print(f" {duration:.3f}s")
    
    avg_time = sum(times) / len(times)
    throughput_mbps = data_size_mb / avg_time
    throughput_gbps = throughput_mbps / 1024
    
    return {
        'test_name': 'sha256_throughput',
        'data_size_mb': data_size_mb,
        'runs': runs,
        'individual_times': times,
        'average_time_seconds': avg_time,
        'throughput_mbps': throughput_mbps,
        'throughput_gbps': throughput_gbps,
        'min_time': min(times),
        'max_time': max(times),
        'std_dev': (sum((t - avg_time) ** 2 for t in times) / len(times)) ** 0.5
    }

def benchmark_rft_performance() -> Dict[str, Any]:
    """Benchmark RFT performance if available"""
    try:
        from encryption.resonance_fourier import resonance_fourier_transform

        # Test with various signal sizes
        signal_sizes = [16, 32, 64, 128, 256, 512]
        results = {}
        
        for size in signal_sizes:
            # Generate test signal
            test_signal = [0.5 + 0.5 * (i / size) for i in range(size)]
            
            # Benchmark RFT
            runs = 1000 if size <= 64 else 100
            start = time.time()
            
            for _ in range(runs):
                resonance_fourier_transform(test_signal)
            
            end = time.time()
            avg_time = (end - start) / runs
            ops_per_second = 1.0 / avg_time
            
            results[f'size_{size}'] = {
                'signal_size': size,
                'runs': runs,
                'avg_time_seconds': avg_time,
                'ops_per_second': ops_per_second,
                'complexity': f'O(N log Ï†) where N={size}'
            }
        
        return {
            'test_name': 'rft_performance',
            'status': 'available',
            'results': results
        }
        
    except ImportError:
        return {
            'test_name': 'rft_performance',
            'status': 'not_available',
            'reason': 'RFT modules not imported'
        }

def benchmark_geometric_hash_performance() -> Dict[str, Any]:
    """Benchmark geometric waveform hashing if available"""
    try:
        from encryption.geometric_waveform_hash import geometric_waveform_hash

        # Test with various waveform sizes
        waveform_sizes = [8, 16, 32, 64, 128]
        results = {}
        
        for size in waveform_sizes:
            # Generate test waveform
            test_waveform = [0.5 + 0.3 * (i / size) for i in range(size)]
            
            # Benchmark geometric hash
            runs = 10000 if size <= 32 else 1000
            start = time.time()
            
            for _ in range(runs):
                geometric_waveform_hash(test_waveform)
            
            end = time.time()
            avg_time = (end - start) / runs
            ops_per_second = 1.0 / avg_time
            
            results[f'size_{size}'] = {
                'waveform_size': size,
                'runs': runs,
                'avg_time_seconds': avg_time,
                'ops_per_second': ops_per_second,
                'golden_ratio_optimized': True
            }
        
        return {
            'test_name': 'geometric_hash_performance',
            'status': 'available',
            'results': results
        }
        
    except ImportError:
        return {
            'test_name': 'geometric_hash_performance',
            'status': 'not_available',
            'reason': 'Geometric hash modules not imported'
        }

def benchmark_quantum_simulation() -> Dict[str, Any]:
    """Benchmark quantum state simulation"""
    try:
        from protected.quantum_engine import QuantumEngine

        # Test quantum state operations
        qubits = [1, 2, 3, 4, 5]
        results = {}
        
        for num_qubits in qubits:
            quantum_engine = QuantumEngine(num_qubits)
            
            # Benchmark state preparation
            runs = 1000 if num_qubits <= 3 else 100
            start = time.time()
            
            for _ in range(runs):
                quantum_engine.hadamard_gate(0)
                quantum_engine.measure()
            
            end = time.time()
            avg_time = (end - start) / runs
            ops_per_second = 1.0 / avg_time
            
            results[f'qubits_{num_qubits}'] = {
                'num_qubits': num_qubits,
                'runs': runs,
                'avg_time_seconds': avg_time,
                'ops_per_second': ops_per_second,
                'state_size': 2 ** num_qubits
            }
        
        return {
            'test_name': 'quantum_simulation_performance',
            'status': 'available',
            'results': results
        }
        
    except ImportError:
        return {
            'test_name': 'quantum_simulation_performance',
            'status': 'not_available',
            'reason': 'Quantum engine modules not imported'
        }

def generate_benchmark_report() -> Dict[str, Any]:
    """Generate comprehensive benchmark report"""
    print("=" * 60)
    print("QUANTONIUM OS AUTOMATED THROUGHPUT BENCHMARK")
    print("=" * 60)
    
    # System information
    system_info = {
        'timestamp': datetime.now().isoformat(),
        'python_version': sys.version,
        'platform': os.name,
        'environment': 'CI/CD' if os.environ.get('CI') else 'local'
    }
    
    # Run benchmarks
    benchmarks = {}
    
    # SHA-256 throughput (always available)
    benchmarks['sha256'] = benchmark_sha256_throughput()
    
    # RFT performance (if available)
    benchmarks['rft'] = benchmark_rft_performance()
    
    # Geometric hash performance (if available)
    benchmarks['geometric_hash'] = benchmark_geometric_hash_performance()
    
    # Quantum simulation performance (if available)
    benchmarks['quantum_simulation'] = benchmark_quantum_simulation()
    
    # Compile final report
    report = {
        'system_info': system_info,
        'benchmarks': benchmarks,
        'summary': {
            'total_benchmarks': len(benchmarks),
            'available_benchmarks': sum(1 for b in benchmarks.values() 
                                      if b.get('status') != 'not_available'),
            'patent_application': 'USPTO #19/169,399',
            'validation_status': 'automated_ci_benchmark'
        }
    }
    
    return report

def main():
    """Main benchmark execution"""
    report = generate_benchmark_report()
    
    # Save detailed report
    with open('benchmark_throughput_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    # Print summary
    print("\n" + "=" * 60)
    print("BENCHMARK SUMMARY")
    print("=" * 60)
    
    sha256_result = report['benchmarks']['sha256']
    print(f"SHA-256 Throughput: {sha256_result['throughput_gbps']:.3f} GB/s")
    print(f"Average Time: {sha256_result['average_time_seconds']:.3f}s")
    print(f"Test Data Size: {sha256_result['data_size_mb']}MB")
    
    # Print availability status
    for name, benchmark in report['benchmarks'].items():
        if name != 'sha256':
            status = benchmark.get('status', 'unknown')
            print(f"{name.upper()}: {status.upper()}")
    
    
    # Create CSV file for CI artifact
    csv_data = []
    csv_data.append(["metric", "value", "unit", "timestamp"])
    csv_data.append(["sha256_throughput", sha256_results["throughput_gbps"], "GB/s", datetime.now().isoformat()])
    csv_data.append(["sha256_avg_time", sha256_results["average_time_seconds"], "seconds", datetime.now().isoformat()])
    csv_data.append(["sha256_data_size", sha256_results["data_size_mb"], "MB", datetime.now().isoformat()])
    csv_data.append(["rft_available", 1 if rft_results["status"] == "available" else 0, "boolean", datetime.now().isoformat()])
    csv_data.append(["geometric_hash_available", 1 if geometric_results["status"] == "available" else 0, "boolean", datetime.now().isoformat()])
    csv_data.append(["quantum_simulation_available", 1 if quantum_results["status"] == "available" else 0, "boolean", datetime.now().isoformat()])
    csv_data.append(["test_runs", sha256_results["runs"], "count", datetime.now().isoformat()])
    csv_data.append(["python_version", sys.version.split()[0], "version", datetime.now().isoformat()])
    
    # Write CSV
    csv_path = "throughput_results.csv"
    with open(csv_path, "w") as f:
        for row in csv_data:
            f.write(",".join(str(x) for x in row) + "\n")
    
    print(f"\nReport saved to: benchmark_throughput_report.json")
    print(f"CSV saved to: {csv_path}")
    print(f"Patent Application: {report['summary']['patent_application']}")
    
    # Exit with success
    return 0

if __name__ == "__main__":
    sys.exit(main())