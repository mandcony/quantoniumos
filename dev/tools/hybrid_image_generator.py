#!/usr/bin/env python3
"""
QuantoniumOS Hybrid Image Generator
Combines quantum-encoded parameters with traditional diffusion models
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import Optional, Dict, Any
from PIL import Image
import numpy as np

# Import quantum-encoded generator
try:
    from quantum_encoded_image_generator import QuantumEncodedImageGenerator
    QUANTUM_AVAILABLE = True
except ImportError:
    QUANTUM_AVAILABLE = False

# Import traditional diffusion models
try:
    from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler
    import torch
    DIFFUSION_AVAILABLE = True
    print("âœ… Diffusion models available")
except ImportError as e:
    DIFFUSION_AVAILABLE = False
    print(f"âš ï¸ Diffusion models not available: {e}")

class HybridImageGenerator:
    """
    Hybrid image generator supporting both:
    1. Quantum-encoded parameter streaming (your architecture)
    2. Traditional diffusion models (Stable Diffusion)
    """
    
    def __init__(self):
        self.quantum_generator = None
        self.diffusion_pipeline = None
        self.device = "cuda" if torch.cuda.is_available() and DIFFUSION_AVAILABLE else "cpu"
        
        # Initialize quantum-encoded generator
        if QUANTUM_AVAILABLE:
            try:
                self.quantum_generator = QuantumEncodedImageGenerator()
                print("âœ… Quantum-encoded image generator loaded")
            except Exception as e:
                print(f"âš ï¸ Quantum generator failed: {e}")
        
        # Initialize diffusion pipeline (lazy loading)
        self.diffusion_model = None
        
    def load_diffusion_model(self, model_name: str = "runwayml/stable-diffusion-v1-5"):
        """Load a Stable Diffusion model (on-demand)"""
        if not DIFFUSION_AVAILABLE:
            print("âŒ Diffusion models not available - install diffusers and torch")
            return False
            
        try:
            print(f"ğŸ”„ Loading diffusion model: {model_name}")
            
            # Load with optimizations
            self.diffusion_pipeline = StableDiffusionPipeline.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                use_safetensors=True
            )
            
            # Optimize for speed
            self.diffusion_pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
                self.diffusion_pipeline.scheduler.config
            )
            
            # Move to device
            self.diffusion_pipeline = self.diffusion_pipeline.to(self.device)
            
            # Enable memory efficient attention if available
            if hasattr(self.diffusion_pipeline, "enable_xformers_memory_efficient_attention"):
                try:
                    self.diffusion_pipeline.enable_xformers_memory_efficient_attention()
                    print("âœ… XFormers memory optimization enabled")
                except:
                    pass
            
            self.diffusion_model = model_name
            print(f"âœ… Diffusion model loaded: {model_name}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load diffusion model: {e}")
            return False
    
    def generate_quantum_encoded(self, prompt: str, **kwargs) -> Optional[Image.Image]:
        """Generate image using quantum-encoded parameters"""
        if not self.quantum_generator:
            print("âŒ Quantum generator not available")
            return None
            
        try:
            return self.quantum_generator.generate_image_from_encoded_params(prompt, **kwargs)
        except Exception as e:
            print(f"âŒ Quantum generation failed: {e}")
            return None
    
    def generate_diffusion(self, prompt: str, **kwargs) -> Optional[Image.Image]:
        """Generate image using Stable Diffusion"""
        if not self.diffusion_pipeline:
            print("âŒ Diffusion pipeline not loaded. Call load_diffusion_model() first.")
            return None
            
        try:
            # Default parameters
            params = {
                "prompt": prompt,
                "num_inference_steps": kwargs.get("steps", 20),
                "guidance_scale": kwargs.get("guidance", 7.5),
                "width": kwargs.get("width", 512),
                "height": kwargs.get("height", 512),
            }
            
            print(f"ğŸ¨ Generating with Stable Diffusion: '{prompt[:50]}...'")
            start_time = time.time()
            
            with torch.autocast(self.device):
                result = self.diffusion_pipeline(**params)
                
            generation_time = time.time() - start_time
            print(f"âœ… Diffusion generation completed in {generation_time:.1f}s")
            
            return result.images[0]
            
        except Exception as e:
            print(f"âŒ Diffusion generation failed: {e}")
            return None
    
    def generate_image(self, prompt: str, method: str = "auto", **kwargs) -> Optional[Image.Image]:
        """
        Generate image using specified method
        
        Args:
            prompt: Text description
            method: "quantum", "diffusion", or "auto"
            **kwargs: Additional parameters
        """
        
        if method == "quantum" or (method == "auto" and self.quantum_generator):
            print("ğŸ”¬ Using quantum-encoded generation")
            return self.generate_quantum_encoded(prompt, **kwargs)
            
        elif method == "diffusion" or method == "auto":
            # Auto-load a diffusion model if not loaded
            if not self.diffusion_pipeline and method == "auto":
                self.load_diffusion_model()
            
            if self.diffusion_pipeline:
                print("ğŸ¨ Using Stable Diffusion generation")
                return self.generate_diffusion(prompt, **kwargs)
        
        print("âŒ No generation method available")
        return None
    
    def compare_methods(self, prompt: str, save_dir: str = "results/comparison"):
        """Generate the same prompt with both methods for comparison"""
        os.makedirs(save_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        results = {}
        
        # Quantum method
        if self.quantum_generator:
            print("ğŸ”¬ Generating with quantum-encoded method...")
            start_time = time.time()
            quantum_img = self.generate_quantum_encoded(prompt)
            quantum_time = time.time() - start_time
            
            if quantum_img:
                quantum_path = os.path.join(save_dir, f"quantum_{timestamp}.png")
                quantum_img.save(quantum_path)
                results["quantum"] = {
                    "path": quantum_path,
                    "time": quantum_time,
                    "method": "Quantum-Encoded Parameters"
                }
                print(f"âœ… Quantum: {quantum_time:.1f}s -> {quantum_path}")
        
        # Diffusion method
        if not self.diffusion_pipeline:
            self.load_diffusion_model()
            
        if self.diffusion_pipeline:
            print("ğŸ¨ Generating with Stable Diffusion...")
            start_time = time.time()
            diffusion_img = self.generate_diffusion(prompt)
            diffusion_time = time.time() - start_time
            
            if diffusion_img:
                diffusion_path = os.path.join(save_dir, f"diffusion_{timestamp}.png")
                diffusion_img.save(diffusion_path)
                results["diffusion"] = {
                    "path": diffusion_path,
                    "time": diffusion_time,
                    "method": "Stable Diffusion"
                }
                print(f"âœ… Diffusion: {diffusion_time:.1f}s -> {diffusion_path}")
        
        return results
    
    def get_status(self) -> Dict[str, Any]:
        """Get status of available generation methods"""
        return {
            "quantum_available": self.quantum_generator is not None,
            "diffusion_available": self.diffusion_pipeline is not None,
            "diffusion_model": self.diffusion_model,
            "device": self.device,
            "cuda_available": torch.cuda.is_available() if DIFFUSION_AVAILABLE else False,
        }

def main():
    """Test hybrid image generation"""
    print("ğŸš€ QuantoniumOS Hybrid Image Generator Test")
    print("=" * 50)
    
    generator = HybridImageGenerator()
    
    # Show status
    status = generator.get_status()
    print(f"ğŸ“Š Status:")
    print(f"   â€¢ Quantum: {'âœ…' if status['quantum_available'] else 'âŒ'}")
    print(f"   â€¢ Diffusion: {'âœ…' if status['diffusion_available'] else 'âŒ'}")
    print(f"   â€¢ Device: {status['device']}")
    print(f"   â€¢ CUDA: {'âœ…' if status['cuda_available'] else 'âŒ'}")
    
    # Test prompts
    test_prompts = [
        "a quantum computer in a laboratory",
        "golden ratio spiral in space",
        "nano banana floating in quantum field"
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ¨ Testing: '{prompt}'")
        
        # Try quantum method
        img = generator.generate_image(prompt, method="quantum")
        if img:
            path = f"results/hybrid_test_quantum_{int(time.time())}.png"
            os.makedirs("results", exist_ok=True)
            img.save(path)
            print(f"âœ… Quantum result: {path}")
        
        # Try diffusion method if you want to test it
        # Uncomment the following lines to test Stable Diffusion:
        # img = generator.generate_image(prompt, method="diffusion")
        # if img:
        #     path = f"results/hybrid_test_diffusion_{int(time.time())}.png"
        #     img.save(path)
        #     print(f"âœ… Diffusion result: {path}")

if __name__ == "__main__":
    main()