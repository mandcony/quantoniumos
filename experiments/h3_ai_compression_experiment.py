#!/usr/bin/env python3
# SPDX-License-Identifier: AGPL-3.0-only
# Copyright (C) 2026 Luis M. Minier / quantoniumos
"""
H3 Cascade AI Compression Experiment
=====================================

GOAL: Test if neural network weights can be compressed with H3 Hierarchical Cascade
      at a REASONABLE ratio (10x-50x) and still produce coherent output.

This is an HONEST scientific experiment:
- We compress at multiple ratios
- We test inference at each ratio
- We measure degradation quantitatively

Author: QuantoniumOS Team
Date: December 21, 2025
"""

import sys
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

# Setup paths
PROJECT_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJECT_ROOT))

# Check dependencies
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("‚ùå PyTorch/Transformers not available. Install with: pip install torch transformers")

from algorithms.rft.hybrids.cascade_hybrids import H3HierarchicalCascade


@dataclass
class CompressionResult:
    """Results from compression experiment"""
    sparsity_target: float
    actual_compression_ratio: float
    original_size_mb: float
    compressed_size_mb: float
    reconstruction_error_mean: float
    reconstruction_error_max: float
    inference_test_passed: bool
    sample_output: str
    time_compress_s: float
    time_decompress_s: float


class H3ModelCompressor:
    """
    Compress neural network weights using H3 Hierarchical Cascade.
    
    Unlike the broken 9000:1 compression, this uses REASONABLE ratios
    that might actually preserve model functionality.
    """
    
    def __init__(self):
        self.cascade = H3HierarchicalCascade()
        self.compressed_layers: Dict[str, dict] = {}
        self.metadata: Dict = {}
        
    def compress_model(self, model, sparsity: float = 0.90, max_params_per_layer: Optional[int] = None, layer_limit: Optional[int] = None) -> Dict[str, np.ndarray]:
        """
        Compress all model weights using H3 cascade.
        
        Args:
            model: PyTorch model
            sparsity: Target sparsity (0.90 = keep 10% of coefficients = ~10x compression)
        
        Returns:
            Dictionary of compressed weight tensors
        """
        compressed = {}
        total_original = 0
        total_compressed = 0
        
        print(f"\n‚öõÔ∏è Compressing with H3 Cascade (sparsity={sparsity:.2f})")
        print("=" * 60)
        
        for i, (name, param) in enumerate(model.named_parameters()):
            if layer_limit is not None and i >= layer_limit:
                print(f"  Skipping remaining layers (layer_limit={layer_limit})")
                break

            tensor = param.detach().cpu().numpy()
            original_shape = tensor.shape
            if max_params_per_layer is not None and tensor.size > max_params_per_layer:
                tensor = tensor.flatten()[:max_params_per_layer].reshape(-1)
                print(f"  {name}: truncated to {tensor.size} params for fast experiment")
            original_size = original_shape[0] * 4 if tensor.ndim == 1 and len(original_shape)==1 else np.prod(original_shape)*4
            total_original += original_size
            
            # Flatten for H3 processing
            flat = tensor.flatten().astype(np.float64)
            
            # Apply H3 cascade
            t_layer_start = time.perf_counter()
            result = self.cascade.encode(flat, sparsity=sparsity)
            t_layer_elapsed = time.perf_counter() - t_layer_start
            
            # Store sparse coefficients (only non-zero)
            nonzero_mask = result.coefficients != 0
            nonzero_indices = np.where(nonzero_mask)[0]
            nonzero_values = result.coefficients[nonzero_mask]
            
            compressed_size = len(nonzero_indices) * 8 + len(nonzero_values) * 16  # indices + complex values
            total_compressed += compressed_size
            
            compressed[name] = {
                'shape': tensor.shape,
                'original_shape': original_shape,
                'truncated': tensor.size != np.prod(original_shape),
                'dtype': str(tensor.dtype),
                'original_length': len(flat),
                'coeff_length': len(result.coefficients),
                'indices': nonzero_indices,
                'values_real': np.real(nonzero_values),
                'values_imag': np.imag(nonzero_values),
                'bpp': result.bpp,
                'sparsity': result.sparsity
            }
            
            ratio = original_size / max(compressed_size, 1)
            print(f"  {name}: {tensor.shape} ‚Üí {ratio:.1f}x ({result.bpp:.2f} BPP) in {t_layer_elapsed:.2f}s")
        
        self.metadata = {
            'sparsity_target': sparsity,
            'total_original_bytes': total_original,
            'total_compressed_bytes': total_compressed,
            'compression_ratio': total_original / max(total_compressed, 1),
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        print(f"\nüìä Total: {total_original/1e6:.1f} MB ‚Üí {total_compressed/1e6:.1f} MB")
        print(f"   Compression Ratio: {self.metadata['compression_ratio']:.1f}x")
        
        self.compressed_layers = compressed
        return compressed
    
    def decompress_to_state_dict(self, original_model) -> Dict[str, torch.Tensor]:
        """
        Decompress weights back to PyTorch state dict.
        
        Args:
            original_model: Original model (for architecture reference)
            
        Returns:
            State dict with reconstructed weights
        """
        state_dict = {}
        total_error = 0
        max_error = 0
        
        print(f"\nüîÑ Decompressing weights...")
        
        for name, compressed in self.compressed_layers.items():
            # Reconstruct sparse coefficients
            coeff_length = compressed['coeff_length']
            coeffs = np.zeros(coeff_length, dtype=np.complex128)

            indices = compressed['indices']
            values = compressed['values_real'] + 1j * compressed['values_imag']
            coeffs[indices] = values

            # Decode via H3
            original_length = compressed['original_length']
            reconstructed = self.cascade.decode(coeffs, original_length)

            # Reshape; pad if truncated during fast experiment
            shape = tuple(compressed['shape'])
            original_shape = tuple(compressed.get('original_shape', shape))
            truncated = compressed.get('truncated', False)

            reconstructed = reconstructed[:np.prod(shape)].reshape(shape)
            if truncated and np.prod(original_shape) > reconstructed.size:
                pad = np.prod(original_shape) - reconstructed.size
                reconstructed = np.concatenate([reconstructed.flatten(), np.zeros(pad, dtype=reconstructed.dtype)])
                reconstructed = reconstructed.reshape(original_shape)
                shape = original_shape
            else:
                reconstructed = reconstructed.reshape(original_shape)
                shape = original_shape
            
            # Convert to tensor
            state_dict[name] = torch.from_numpy(reconstructed.astype(np.float32))
            
            # Compute error against original
            if name in dict(original_model.named_parameters()):
                original = original_model.state_dict()[name].numpy()
                error = np.abs(reconstructed - original)
                total_error += np.mean(error)
                max_error = max(max_error, np.max(error))
        
        print(f"   Mean reconstruction error: {total_error / len(self.compressed_layers):.6f}")
        print(f"   Max reconstruction error: {max_error:.6f}")
        
        return state_dict
    
    def save(self, path: Path):
        """Save compressed model to disk."""
        # Convert numpy arrays to lists for JSON
        saveable = {}
        for name, data in self.compressed_layers.items():
            shape_list = [int(x) for x in data['shape']]
            orig_shape_list = [int(x) for x in data['original_shape']]
            saveable[name] = {
                'shape': shape_list,
                'original_shape': orig_shape_list,
                'dtype': data['dtype'],
                'original_length': int(data['original_length']),
                'coeff_length': int(data['coeff_length']),
                'indices': [int(x) for x in data['indices'].tolist()],
                'values_real': data['values_real'].tolist(),
                'values_imag': data['values_imag'].tolist(),
                'bpp': float(data['bpp']),
                'sparsity': float(data['sparsity']),
                'truncated': bool(data.get('truncated', False))
            }
        
        meta = {
            'sparsity_target': float(self.metadata.get('sparsity_target', 0.0)),
            'total_original_bytes': int(self.metadata.get('total_original_bytes', 0)),
            'total_compressed_bytes': int(self.metadata.get('total_compressed_bytes', 0)),
            'compression_ratio': float(self.metadata.get('compression_ratio', 0.0)),
            'timestamp': self.metadata.get('timestamp', '')
        }

        output = {
            'metadata': meta,
            'layers': saveable
        }
        
        with open(path, 'w') as f:
            json.dump(output, f)
        
        print(f"‚úÖ Saved to {path} ({path.stat().st_size / 1e6:.1f} MB)")


def test_inference(model, tokenizer, prompt: str = "Hello, how are you?") -> Tuple[bool, str]:
    """
    Test if model can generate coherent text.
    
    Returns:
        (success, output_text)
    """
    try:
        inputs = tokenizer.encode(prompt + tokenizer.eos_token, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=30,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                num_return_sequences=1
            )
        
        response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
        
        # REAL coherence check: look for actual English words
        # Common English words that should appear in coherent text
        common_words = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                       'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                       'should', 'may', 'might', 'must', 'shall', 'can', 'need', 'dare',
                       'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her',
                       'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their',
                       'this', 'that', 'these', 'those', 'what', 'which', 'who', 'whom',
                       'and', 'or', 'but', 'if', 'because', 'as', 'until', 'while',
                       'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
                       'into', 'through', 'during', 'before', 'after', 'above', 'below',
                       'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over',
                       'not', 'no', 'yes', 'so', 'just', 'now', 'then', 'here', 'there',
                       'when', 'where', 'why', 'how', 'all', 'each', 'every', 'both',
                       'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own',
                       'same', 'than', 'too', 'very', 'just', 'also', 'back', 'well',
                       'way', 'even', 'new', 'want', 'because', 'any', 'give', 'day',
                       'good', 'first', 'last', 'long', 'great', 'little', 'own', 'old',
                       'right', 'big', 'high', 'different', 'small', 'large', 'next',
                       'early', 'young', 'important', 'public', 'bad', 'same', 'able'}
        
        words = response.lower().split()
        if len(words) < 3:
            return False, response
        
        # Check if at least 30% of words are recognizable English
        english_word_count = sum(1 for w in words if w.strip('.,!?"\'-:;()[]') in common_words)
        english_ratio = english_word_count / len(words)
        
        # Also check for repetitive garbage (same token repeated)
        unique_words = set(words)
        repetition_ratio = len(unique_words) / len(words)
        
        # Coherent if: >30% English words AND not too repetitive (>20% unique)
        is_coherent = english_ratio > 0.30 and repetition_ratio > 0.20
        
        return is_coherent, response
        
    except Exception as e:
        return False, f"ERROR: {e}"


def run_experiment(sparsity_levels: List[float] = [0.90, 0.95], layer_limit: Optional[int] = 5, max_params_per_layer: Optional[int] = 2_000_000):
    """
    Run the full compression experiment at multiple sparsity levels.
    """
    if not TORCH_AVAILABLE:
        print("Cannot run experiment without PyTorch")
        return
    
    print("=" * 70)
    print("   H3 CASCADE AI COMPRESSION EXPERIMENT")
    print("   Testing Realistic Compression Ratios")
    print("=" * 70)
    
    # Load model
    print("\nüì• Loading DialoGPT-small...")
    model_id = "microsoft/DialoGPT-small"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    original_model = AutoModelForCausalLM.from_pretrained(model_id)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    total_params = sum(p.numel() for p in original_model.parameters())
    print(f"   Parameters: {total_params:,}")
    print(f"   Original size: {total_params * 4 / 1e6:.1f} MB")
    
    # Test original model first
    print("\nüß™ Testing ORIGINAL model...")
    original_passed, original_output = test_inference(original_model, tokenizer)
    print(f"   Output: {original_output[:100]}...")
    print(f"   Coherent: {'‚úÖ YES' if original_passed else '‚ùå NO'}")
    
    # Run experiments
    results: List[CompressionResult] = []
    output_dir = PROJECT_ROOT / "results" / "h3_compression_experiment"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for sparsity in sparsity_levels:
        print(f"\n{'='*70}")
        print(f"   EXPERIMENT: Sparsity = {sparsity:.0%} (keep {(1-sparsity)*100:.0f}% of coefficients)")
        print(f"{'='*70}")
        
        compressor = H3ModelCompressor()
        
        # Compress
        t0 = time.perf_counter()
        compressor.compress_model(
            original_model,
            sparsity=sparsity,
            max_params_per_layer=max_params_per_layer,
            layer_limit=layer_limit,
        )
        compress_time = time.perf_counter() - t0
        
        # Decompress
        t0 = time.perf_counter()
        reconstructed_state_dict = compressor.decompress_to_state_dict(original_model)
        decompress_time = time.perf_counter() - t0
        
        # Load into fresh model
        test_model = AutoModelForCausalLM.from_pretrained(model_id)
        test_model.load_state_dict(reconstructed_state_dict, strict=False)
        test_model.eval()
        
        # Test inference
        print("\nüß™ Testing COMPRESSED model...")
        passed, output = test_inference(test_model, tokenizer)
        print(f"   Output: {output[:100]}...")
        print(f"   Coherent: {'‚úÖ YES' if passed else '‚ùå NO'}")
        
        # Compute reconstruction error
        errors = []
        for name, param in original_model.named_parameters():
            if name not in reconstructed_state_dict:
                continue
            original = param.detach().cpu().numpy()
            reconstructed = reconstructed_state_dict[name].numpy()
            # Align sizes in case of truncation
            if original.size != reconstructed.size:
                min_size = min(original.size, reconstructed.size)
                original_flat = original.flatten()[:min_size]
                reconstructed_flat = reconstructed.flatten()[:min_size]
            else:
                original_flat = original.flatten()
                reconstructed_flat = reconstructed.flatten()
            errors.append(np.abs(original_flat - reconstructed_flat))

        all_errors = np.concatenate([e.flatten() for e in errors]) if errors else np.array([0.0])
        
        result = CompressionResult(
            sparsity_target=sparsity,
            actual_compression_ratio=compressor.metadata['compression_ratio'],
            original_size_mb=compressor.metadata['total_original_bytes'] / 1e6,
            compressed_size_mb=compressor.metadata['total_compressed_bytes'] / 1e6,
            reconstruction_error_mean=float(np.mean(all_errors)),
            reconstruction_error_max=float(np.max(all_errors)),
            inference_test_passed=passed,
            sample_output=output[:200],
            time_compress_s=compress_time,
            time_decompress_s=decompress_time
        )
        results.append(result)
        
        # Save compressed model
        save_path = output_dir / f"dialogpt_h3_sparsity_{int(sparsity*100)}.json"
        compressor.save(save_path)
    
    # Summary
    print("\n" + "=" * 70)
    print("   EXPERIMENT RESULTS SUMMARY")
    print("=" * 70)
    print(f"\n{'Sparsity':<12} {'Ratio':<10} {'Size (MB)':<12} {'Error (mean)':<14} {'Inference':<10}")
    print("-" * 70)
    
    for r in results:
        status = "‚úÖ PASS" if r.inference_test_passed else "‚ùå FAIL"
        print(f"{r.sparsity_target:<12.0%} {r.actual_compression_ratio:<10.1f}x "
              f"{r.compressed_size_mb:<12.1f} {r.reconstruction_error_mean:<14.6f} {status}")
    
    # Save summary
    summary = {
        'experiment': 'H3 Cascade AI Compression',
        'model': 'microsoft/DialoGPT-small',
        'date': time.strftime("%Y-%m-%d %H:%M:%S"),
        'original_inference_passed': original_passed,
        'results': [
            {
                'sparsity': r.sparsity_target,
                'compression_ratio': r.actual_compression_ratio,
                'compressed_size_mb': r.compressed_size_mb,
                'reconstruction_error_mean': r.reconstruction_error_mean,
                'reconstruction_error_max': r.reconstruction_error_max,
                'inference_passed': r.inference_test_passed,
                'sample_output': r.sample_output
            }
            for r in results
        ]
    }
    
    summary_path = output_dir / "experiment_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(summary, f, indent=2)
    
    print(f"\nüìä Results saved to: {output_dir}")
    
    # Scientific conclusion
    print("\n" + "=" * 70)
    print("   SCIENTIFIC CONCLUSION")
    print("=" * 70)
    
    passing_results = [r for r in results if r.inference_test_passed]
    if passing_results:
        best = max(passing_results, key=lambda r: r.actual_compression_ratio)
        print(f"\n‚úÖ MAXIMUM VIABLE COMPRESSION: {best.actual_compression_ratio:.1f}x")
        print(f"   At sparsity={best.sparsity_target:.0%}, model still generates coherent text.")
        print(f"   Size reduction: {best.original_size_mb:.1f} MB ‚Üí {best.compressed_size_mb:.1f} MB")
    else:
        print(f"\n‚ùå NO VIABLE COMPRESSION FOUND")
        print(f"   Even at lowest sparsity ({min(sparsity_levels):.0%}), inference failed.")
        print(f"   This suggests H3 cascade alone cannot preserve model functionality.")
        print(f"   Next step: Try fine-tuning after compression.")
    
    return results


if __name__ == "__main__":
    # Run with more conservative sparsity levels
    run_experiment(sparsity_levels=[0.50, 0.70, 0.80, 0.90, 0.95])
